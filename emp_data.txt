import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions
import org.apache.hadoop.conf._
import org.apache.hadoop.fs._
import java.nio.file.{Paths, Files}
object SparkPii { //classname
  def main(args: Array[String]): Unit = {
    //creating spark configuration
    val conf = new SparkConf().set("spark.speculation", "true").setAppName("EmpDetails").setMaster("yarn-cluster")
    // creating spark context
    val sc = new SparkContext(conf)
    //reading input
    val data = sc.textFile("emp.txt").map(_.split("\\|"))
      .filter(lines => lines(0) != "empno")
      .map(tdata => Emp(tdata(0), tdata(1), tdata(2), tdata(3), tdata(4), tdata(5), tdata(6), tdata(7)))
    //passing output filename as arguements 
      val outputpath = args(0)
    //creating hadoop configuration
    val hadoopConf = new Configuration()
    //connecting to cloudera port
    val hdfs = FileSystem.get(new java.net.URI("hdfs://quickstart.cloudera:8020"), hadoopConf)
    //deleting output file if present
    hdfs.delete(new Path("hdfs://quickstart.cloudera:8020/user/cloudera/", outputpath), true)
    //converting rdd to pairRDD
    val databydeptno: RDD[(String, Emp)] = data.keyBy(x => x.deptno)
    // fetching no. of employees  deptartmentwise
    val data2 = databydeptno.countByKey()
      .map(x => ("Total employees in deptartment " + x._1 + " is " + x._2))
      //fetching commission deptwise
    val data1 = data.map(x => (x.deptno.trim().toInt, x.comm.trim().toDouble)).reduceByKey((a, b) => a + b)
      .map(x => ("Total commission for deptartment Number " + x._1 + " is Rs " + x._2))
    //converting pairRDD to RDD
    val data3 = sc.parallelize(data2.toList)
    //getting union of two diferent RDD
    val data4 = data1.union(data3)
    //saving output in file
    data4.saveAsTextFile(outputpath)
  }
}
case class Emp(empno: String, ename: String, job: String, mgr: String, hiredate: String, sal: String,
comm: String, deptno: String)